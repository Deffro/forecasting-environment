{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58bcca3-24bc-490d-936b-08984803d334",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ce1f6b7-0739-4e0c-90c1-34a407590089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sktime\n",
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.arima import AutoARIMA, ARIMA\n",
    "from sktime.forecasting.compose import MultiplexForecaster, AutoEnsembleForecaster, ColumnEnsembleForecaster, DirRecTabularRegressionForecaster, RecursiveTabularRegressionForecaster, DirRecTimeSeriesRegressionForecaster, DirectTabularRegressionForecaster, DirectTimeSeriesRegressionForecaster, EnsembleForecaster, StackingForecaster\n",
    "from sktime.forecasting.ets import AutoETS\n",
    "from sktime.forecasting.theta import ThetaForecaster\n",
    "from sktime.forecasting.trend import PolynomialTrendForecaster\n",
    "from sktime.transformations.series.compose import ColumnwiseTransformer\n",
    "from sktime.transformations.series.adapt import TabularToSeriesAdaptor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sktime.performance_metrics.forecasting import MeanSquaredError, MeanAbsoluteScaledError, mean_absolute_percentage_error, MeanAbsoluteError\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter, ForecastingGridSearchCV\n",
    "from sktime.forecasting.compose import TransformedTargetForecaster\n",
    "from sktime.transformations.series.detrend import Deseasonalizer, Detrender\n",
    "from sktime.transformations.series.difference import Differencer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, Lars, LassoLars, BayesianRidge, HuberRegressor, PassiveAggressiveRegressor, OrthogonalMatchingPursuit\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "import lightgbm as lgbm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc6122-38e6-4003-aa61-7c1b3bf10a72",
   "metadata": {},
   "source": [
    "## Algorithm parameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92ea75be-04b8-4875-b3d4-fa24591d274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    'decision_tree': {\n",
    "        'estimator': \n",
    "            DecisionTreeRegressor(ccp_alpha=0.0,  criterion='mse', max_depth=None, max_features=None, max_leaf_nodes=None, \n",
    "                                  min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, \n",
    "                                  min_weight_fraction_leaf=0.0, random_state=42, splitter='best')\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__ccp_alpha': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__max_depth': [1, 2, 3, 4, 5, 10, None],\n",
    "            'forecaster__estimator__max_leaf_nodes': [3, 8, 16, 100, None],\n",
    "            'forecaster__estimator__min_impurity_decrease': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__min_samples_leaf': [1, 2, 3, 4],\n",
    "            'forecaster__estimator__min_samples_split': [2, 3]            \n",
    "        }     \n",
    "    },\n",
    "    'random_forest': {\n",
    "        'estimator': \n",
    "            RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', \n",
    "                                  max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                  min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, \n",
    "                                  n_jobs=-1, oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__ccp_alpha': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__max_depth': [1, 2, 3, 4, 5, 10, None],\n",
    "            'forecaster__estimator__max_leaf_nodes': [3, 8, 16, 100, None],\n",
    "            'forecaster__estimator__min_impurity_decrease': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__min_samples_leaf': [1, 2, 3, 4],\n",
    "            'forecaster__estimator__min_samples_split': [1, 2, 3],\n",
    "            'forecaster__estimator__n_estimators': [10, 50, 100, 200],        \n",
    "        }     \n",
    "    },    \n",
    "    'extra_trees': {\n",
    "        'estimator': \n",
    "            ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse', max_depth=None, max_features='auto', \n",
    "                                max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, \n",
    "                                n_jobs=-1, oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__ccp_alpha': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__max_depth': [1, 2, 3, 4, 5, 10, -1],\n",
    "            'forecaster__estimator__max_leaf_nodes': [3, 8, 16, 100, -1],\n",
    "            'forecaster__estimator__min_impurity_decrease': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__min_samples_leaf': [1, 2, 3, 4],\n",
    "            'forecaster__estimator__min_samples_split': [1, 2, 3],\n",
    "            'forecaster__estimator__n_estimators': [10, 50, 100],\n",
    "            'forecaster__estimator__warm_start': [True, False],     \n",
    "        }     \n",
    "    },     \n",
    "    'gradient_boosting': {\n",
    "        'estimator': \n",
    "            GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse', init=None, learning_rate=0.1, loss='ls', \n",
    "                                      max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                      min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, \n",
    "                                      random_state=42, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__alpha': [0.5, 0.9],\n",
    "            'forecaster__estimator__ccp_alpha': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__max_depth': [2, 3, 5, 10, -1],\n",
    "            'forecaster__estimator__min_impurity_decrease': [0, 0.01, 0.1],\n",
    "            'forecaster__estimator__min_samples_leaf': [1, 2],\n",
    "            'forecaster__estimator__min_samples_split': [2, 3],\n",
    "            'forecaster__estimator__n_estimators': [10, 100, 200],\n",
    "            'forecaster__estimator__learning_rate': [0.1, 0.01], \n",
    "        }     \n",
    "    },       \n",
    "    'adaboost': {\n",
    "        'estimator': \n",
    "            AdaBoostRegressor(base_estimator=None, learning_rate=1.0, loss='linear', n_estimators=50, random_state=42)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__loss': ['linear', 'square', 'exponential'],\n",
    "            'forecaster__estimator__n_estimators': [10, 50, 100, 200],\n",
    "            'forecaster__estimator__learning_rate': [0.1, 0.05, 0.01],\n",
    "        }     \n",
    "    },      \n",
    "    'lgb_regressor': {\n",
    "        'estimator': \n",
    "            lgbm.sklearn.LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0, importance_type='split', learning_rate=0.1, max_depth=-1, \n",
    "                                       min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=None, \n",
    "                                       random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent='warn', subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__max_depth': [1, 2, 3, 4, 5, 10, -1],\n",
    "            'forecaster__estimator__num_leaves': [2, 3, 10, 20, 31, 100],\n",
    "            'forecaster__estimator__min_child_samples': [5, 10, 20, 50],\n",
    "            'forecaster__estimator__min_child_weight': [0.001, 0.005],\n",
    "            'forecaster__estimator__n_estimators': [10, 50, 100, 200],\n",
    "        }     \n",
    "    },   \n",
    "    'knn': {\n",
    "        'estimator': \n",
    "            KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=-1, n_neighbors=5, p=2, weights='uniform')\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__n_neighbors': [3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "            'forecaster__estimator__p': [1, 2, 3],\n",
    "        }     \n",
    "    },    \n",
    "    'passive_aggressive': {\n",
    "        'estimator': \n",
    "            PassiveAggressiveRegressor(C=1.0, average=False, early_stopping=False, epsilon=0.1, fit_intercept=True, loss='epsilon_insensitive', max_iter=1000, \n",
    "                                       n_iter_no_change=5, random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__C': [0.1, 0.25, 0.5, 0.75, 1],\n",
    "            'forecaster__estimator__early_stopping': [True, False],\n",
    "            'forecaster__estimator__epsilon': [0.01, 0.05, 0.1, 0.2],\n",
    "            'forecaster__estimator__max_iter': [500, 1000, 2000],\n",
    "            'forecaster__estimator__n_iter_no_change': [1, 2, 3, 4, 5, 7],\n",
    "            'forecaster__estimator__validation_fraction': [0.1, 0.2],\n",
    "            'forecaster__estimator__tol': [None, 0.001, 0.002],\n",
    "        }     \n",
    "    },       \n",
    "    'huber': {\n",
    "        'estimator': \n",
    "            HuberRegressor(alpha=0.0001, epsilon=1.35, fit_intercept=True, max_iter=100, tol=1e-05, warm_start=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__alpha': [0.00005, 0.0001, 0.0005, 0.001],\n",
    "            'forecaster__estimator__early_epsilon': [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2],\n",
    "            'forecaster__estimator__max_iter': [50, 100, 200, 500],\n",
    "            'forecaster__estimator__tol': [1e-05, 1e-06, 5e-05, 5e-04],\n",
    "            'forecaster__estimator__warm_start': [True, False],\n",
    "        }     \n",
    "    },     \n",
    "    'bayesian_ridge': {\n",
    "        'estimator': \n",
    "            BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, alpha_init=None, compute_score=False, copy_X=True, fit_intercept=True, lambda_1=1e-06, \n",
    "                          lambda_2=1e-06, lambda_init=None, n_iter=300, normalize=False, tol=0.001, verbose=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__alpha_1': [1e-05, 5e-05, 1e-06, 5e-06],\n",
    "            'forecaster__estimator__alpha_2': [1e-05, 5e-05, 1e-06, 5e-06],\n",
    "            'forecaster__estimator__lambda_1': [1e-05, 5e-05, 1e-06, 5e-06],\n",
    "            'forecaster__estimator__lambda_2': [1e-05, 5e-05, 1e-06, 5e-06],\n",
    "            'forecaster__estimator__compute_score': [True, False],\n",
    "            'forecaster__estimator__n_iter': [100, 200, 300, 400],\n",
    "            'forecaster__estimator__tol': [0.0005, 0.001, 0.005, 0.01, 0.05],\n",
    "        }     \n",
    "    },        \n",
    "    'lasso_lars': {\n",
    "        'estimator': \n",
    "            LassoLars(alpha=1.0, copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True, jitter=None, max_iter=500, \n",
    "                      normalize=True, positive=False, precompute='auto', random_state=42, verbose=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2],\n",
    "            'forecaster__estimator__max_iter': [10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "        }     \n",
    "    },        \n",
    "    'lars': {\n",
    "        'estimator': \n",
    "            Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True, jitter=None, n_nonzero_coefs=500, \n",
    "                 normalize=True, precompute='auto', random_state=42, verbose=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__n_nonzero_coefs': [1, 5, 10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "        }     \n",
    "    },       \n",
    "    'elastic_net': {\n",
    "        'estimator': \n",
    "            ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5, max_iter=1000, normalize=False, positive=False, \n",
    "                       precompute=False, random_state=42, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2],\n",
    "            'forecaster__estimator__l1_ratio': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "            'forecaster__estimator__max_iter': [10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            'forecaster__estimator__tol': [0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005],\n",
    "            'forecaster__estimator__warm_start': [True, False],\n",
    "        }     \n",
    "    },        \n",
    "    'ridge': {\n",
    "        'estimator': \n",
    "            Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=42, solver='auto', tol=0.001)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2],\n",
    "            'forecaster__estimator__max_iter': [10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            'forecaster__estimator__tol': [0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005],\n",
    "        }     \n",
    "    },     \n",
    "    'lasso': {\n",
    "        'estimator': \n",
    "            Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, \n",
    "                  random_state=42, selection='cyclic', tol=0.0001, warm_start=False)\n",
    "        ,\n",
    "        'params': {\n",
    "            'forecaster__estimator__alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2],\n",
    "            'forecaster__estimator__max_iter': [10, 50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "            'forecaster__estimator__tol': [0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00005],\n",
    "            'forecaster__estimator__warm_start': [True, False],\n",
    "        }     \n",
    "    },        \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc14ca8-13d7-4d34-bc97-7b3db5a56c57",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc046990-f317-43bd-906e-4ca455adfef5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'date' was removed from Satellite.\n",
      "###################################################################### sla ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "###################################################################### ugosa ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "###################################################################### vgosa ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "###################################################################### err_ugosa ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "###################################################################### err_vgosa ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "###################################################################### adt ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "###################################################################### ugos ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "###################################################################### vgos ######################################################################\n",
      "train datetime margins              : 1993-01 - 2017-12.     Total samples: 300 (89.3%)\n",
      "test datetime margins               : 2018-01 - 2020-12.     Total samples: 36 (10.7%)\n",
      "valid datetime margins              : 2015-01 - 2017-12.     Total samples: 36 (10.7%)\n",
      "train_without_valid datetime margins: 1993-01 - 2014-12.     Total samples: 264 (78.6%)\n",
      "decision_tree\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "random_forest\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "extra_trees\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "gradient_boosting\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "adaboost\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lgb_regressor\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "knn\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "passive_aggressive\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "huber\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "bayesian_ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso_lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lars\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "elastic_net\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "ridge\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n",
      "lasso\n",
      "Fitting 36 folds for each of 4 candidates, totalling 144 fits\n"
     ]
    }
   ],
   "source": [
    "tune_only_window_length = True\n",
    "fast = False\n",
    "\n",
    "# define forecastin horizon\n",
    "fh = 1\n",
    "\n",
    "# Read Data\n",
    "dataset_name = 'Satellite'\n",
    "data, seasonal_period, freq_sktime = read_file(dataset_name, data_path='H:/My Drive/PhD/ECOSCOPE/time-series-forecasting-waves/data/')\n",
    "preprocess = False\n",
    "\n",
    "# ONLY FOR SKTIME\n",
    "# keep datetime as a column for plots\n",
    "data['datetime'] = data.index\n",
    "data.index = pd.PeriodIndex(data.index, freq=freq_sktime)\n",
    "\n",
    "# metric\n",
    "mase = MeanAbsoluteScaledError(sp=seasonal_period)\n",
    "\n",
    "for target in data.drop(columns=['datetime']):\n",
    "    print('#'*70, target, '#'*70)\n",
    "\n",
    "    # split data\n",
    "    train, test, valid, train_without_valid, train_test_split_date, train_valid_split_date = train_valid_test_split(dataset_name, data)\n",
    "\n",
    "    if fast is True:\n",
    "        initial_window = train[:train.shape[0]-seasonal_period].shape[0]\n",
    "    else:\n",
    "        initial_window = train_without_valid.shape[0]\n",
    "\n",
    "    # expanding window to fit test data\n",
    "    cv = ExpandingWindowSplitter(step_length=1, fh=fh, initial_window=initial_window)\n",
    "    min_max_scaler = TabularToSeriesAdaptor(MinMaxScaler(feature_range=(1, 2)))\n",
    "\n",
    "    for algorithm_name, value in algorithms.items():\n",
    "        print(algorithm_name)\n",
    "\n",
    "        estimator = DirectTabularRegressionForecaster(estimator=value['estimator'])\n",
    "\n",
    "        pipe = TransformedTargetForecaster(steps=[\n",
    "            # (\"detrender\", Detrender()),\n",
    "            # (\"deseasonalizer\", Differencer(lags=1)),\n",
    "            (\"minmaxscaler\", min_max_scaler),\n",
    "            (\"forecaster\", estimator),\n",
    "        ])\n",
    "\n",
    "        if seasonal_period == 7:\n",
    "            window_size = 1\n",
    "        elif seasonal_period == 12 or seasonal_period == 24:\n",
    "            window_size = seasonal_period\n",
    "        \n",
    "        if tune_only_window_length is True:\n",
    "            param_grid = {\"forecaster__window_length\": [i*seasonal_period for i in range(1,5)]}\n",
    "        else:\n",
    "            param_grid = value['params']\n",
    "            param_grid['forecaster__window_length'] = [i*seasonal_period for i in range(1,5)]\n",
    "\n",
    "        gscv = ForecastingGridSearchCV(\n",
    "            forecaster = pipe, \n",
    "            strategy = \"refit\", \n",
    "            cv = cv, \n",
    "            param_grid = param_grid,\n",
    "            scoring = mase,\n",
    "            n_jobs = -2,\n",
    "            verbose=2\n",
    "        )\n",
    "\n",
    "        gscv.fit(train[target], fh=fh)\n",
    "\n",
    "        # Save models\n",
    "        # if tune_only_window_length is True:\n",
    "            # pd.to_pickle(gscv.best_forecaster_, f'../../results/tuned_models/just_window/{dataset_name}/{target}.{algorithm_name}.pkl')\n",
    "        # else:\n",
    "            # pd.to_pickle(gscv.best_forecaster_, f'../../results/tuned_models/window_and_algorithm/{dataset_name}/{target}.{algorithm_name}.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff8938d-a9f9-4149-982c-1c42d0fa75eb",
   "metadata": {},
   "source": [
    "## Read saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a5fd778-e372-4968-9420-a6cea80b069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pd.read_pickle(f'../../results/tuned_models/just_window/{dataset_name}/{target}.decision_tree.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69b5b209-dca2-44f0-a689-e7ed700384af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': [('minmaxscaler',\n",
       "   TabularToSeriesAdaptor(transformer=MinMaxScaler(feature_range=(1, 2)))),\n",
       "  ('forecaster',\n",
       "   DirectTabularRegressionForecaster(estimator=DecisionTreeRegressor(random_state=42),\n",
       "                                     window_length=24))],\n",
       " 'minmaxscaler': TabularToSeriesAdaptor(transformer=MinMaxScaler(feature_range=(1, 2))),\n",
       " 'forecaster': DirectTabularRegressionForecaster(estimator=DecisionTreeRegressor(random_state=42),\n",
       "                                   window_length=24),\n",
       " 'minmaxscaler__transformer__clip': False,\n",
       " 'minmaxscaler__transformer__copy': True,\n",
       " 'minmaxscaler__transformer__feature_range': (1, 2),\n",
       " 'minmaxscaler__transformer': MinMaxScaler(feature_range=(1, 2)),\n",
       " 'forecaster__estimator__ccp_alpha': 0.0,\n",
       " 'forecaster__estimator__criterion': 'mse',\n",
       " 'forecaster__estimator__max_depth': None,\n",
       " 'forecaster__estimator__max_features': None,\n",
       " 'forecaster__estimator__max_leaf_nodes': None,\n",
       " 'forecaster__estimator__min_impurity_decrease': 0.0,\n",
       " 'forecaster__estimator__min_impurity_split': None,\n",
       " 'forecaster__estimator__min_samples_leaf': 1,\n",
       " 'forecaster__estimator__min_samples_split': 2,\n",
       " 'forecaster__estimator__min_weight_fraction_leaf': 0.0,\n",
       " 'forecaster__estimator__random_state': 42,\n",
       " 'forecaster__estimator__splitter': 'best',\n",
       " 'forecaster__estimator': DecisionTreeRegressor(random_state=42),\n",
       " 'forecaster__window_length': 24}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d6dc62b-43b1-4215-bd91-22805617e59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()['forecaster'].get_params()['window_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596468de-16dc-436a-819a-02762c12f67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
